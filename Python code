import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import learning_curve

# Load the dataset
file_path = "exercise_dataset.csv"
data = pd.read_csv(file_path)

# Drop the ID column
data_cleaned = data.drop(columns=['ID'])

# Convert categorical columns to one-hot encoding
categorical_columns = ['Exercise', 'Gender', 'Weather Conditions']
encoder = OneHotEncoder(drop='first', sparse=False)
encoded_categorical_data = pd.DataFrame(
    encoder.fit_transform(data_cleaned[categorical_columns]),
    columns=encoder.get_feature_names_out(categorical_columns)
)

# Drop original categorical columns and add the encoded ones
data_cleaned = data_cleaned.drop(columns=categorical_columns)
data_cleaned = pd.concat([data_cleaned, encoded_categorical_data], axis=1)

# Split the data into features (X) and target (y)
X = data_cleaned.drop(columns=['Actual Weight'])  # Predicting 'Actual Weight' as the target
y = data_cleaned['Actual Weight']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest Regressor model
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions and evaluate the model
y_pred = rf_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Visualization 1: Heatmap (Correlation Matrix)
plt.figure(figsize=(10, 8))
corr_matrix = data_cleaned.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Heatmap of Correlation Matrix')
plt.show()

# Visualization 2: Pair Plot (Relationship Between Variables)
sns.pairplot(data_cleaned)
plt.title('Pair Plot of Variables')
plt.show()

# Visualization 3: Feature Importance Plot
importances = rf_model.feature_importances_
feature_names = X_train.columns
sorted_indices = importances.argsort()

plt.barh(range(len(sorted_indices)), importances[sorted_indices], align='center')
plt.yticks(range(len(sorted_indices)), feature_names[sorted_indices])
plt.xlabel('Feature Importance')
plt.title('Feature Importance for Weight Prediction')
plt.show()

# Visualization 4: Residual Plot
residuals = y_test - y_pred
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot for Weight Prediction')
plt.show()

# Visualization 5: Learning Curve
train_sizes, train_scores, test_scores = learning_curve(rf_model, X_train, y_train, cv=5)
train_scores_mean = np.mean(train_scores, axis=1)
test_scores_mean = np.mean(test_scores, axis=1)

plt.plot(train_sizes, train_scores_mean, label='Training score')
plt.plot(train_sizes, test_scores_mean, label='Cross-validation score')
plt.xlabel('Training set size')
plt.ylabel('Score')
plt.legend(loc='best')
plt.title('Learning Curve for Weight Prediction Model')
plt.show()


# Feature importance plot
feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)
feature_importances.nlargest(10).plot(kind='barh')
plt.show()
